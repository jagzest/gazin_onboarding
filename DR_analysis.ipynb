{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8e6e2be-5263-4741-8098-8e92a526423e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "import model_engine\n",
    "import boto3\n",
    "import  numpy as np\n",
    "from functions_for_onboarding import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1316c13-61b1-4bc7-819c-37db24e0c7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List, Set, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from feature_engine_parts.fe_parts_V2.mappers import converters\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Converter registry + helpers\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "CONVERTER_REGISTRY = {\n",
    "    \"MappingBase\": converters.MappingBase,\n",
    "    \"DateConverterV2\": converters.DateConverterV2,\n",
    "    \"NumericConverterV2\": converters.NumericConverterV2,\n",
    "    \"StringConverterV2\": converters.StringConverterV2,\n",
    "}\n",
    "\n",
    "\n",
    "def _build_converter(spec: Dict[str, Any]):\n",
    "    if not isinstance(spec, dict):\n",
    "        raise TypeError(f\"spec must be a dict, got {type(spec)}\")\n",
    "\n",
    "    type_name = spec.get(\"type\")\n",
    "    params = spec.get(\"params\", {}) or {}\n",
    "\n",
    "    if type_name not in CONVERTER_REGISTRY:\n",
    "        known = \", \".join(sorted(CONVERTER_REGISTRY.keys()))\n",
    "        raise ValueError(f\"Unknown converter type '{type_name}'. Known types: {known}\")\n",
    "\n",
    "    if not isinstance(params, dict):\n",
    "        raise TypeError(f\"spec['params'] must be a dict, got {type(params)}\")\n",
    "\n",
    "    cls = CONVERTER_REGISTRY[type_name]\n",
    "    return type_name, cls(**params)\n",
    "\n",
    "\n",
    "def apply_converter_spec(df: pd.DataFrame, spec: Dict[str, Any], *, verbose: bool = True) -> pd.DataFrame:\n",
    "    type_name, converter = _build_converter(spec)\n",
    "\n",
    "    if verbose:\n",
    "        raw_feature = getattr(converter, \"raw_feature\", None)\n",
    "        new_feature = getattr(converter, \"new_feature\", None)\n",
    "        print(f\"[apply_converter_spec] Using {type_name} | raw_feature={raw_feature!r} -> new_feature={new_feature!r}\")\n",
    "\n",
    "    return converter.transform(df)\n",
    "\n",
    "\n",
    "def _output_feature_name_from_spec(spec: Dict[str, Any]) -> str | None:\n",
    "    params = spec.get(\"params\", {}) or {}\n",
    "    return params.get(\"new_feature\") or params.get(\"raw_feature\")\n",
    "\n",
    "\n",
    "def build_output_feature_allowlist(specs: List[Dict[str, Any]]) -> Set[str]:\n",
    "    out: Set[str] = set()\n",
    "    for spec in specs:\n",
    "        name = _output_feature_name_from_spec(spec)\n",
    "        if name:\n",
    "            out.add(name)\n",
    "    return out\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# 1) Synthetic DATE_OF_REQUEST generator based on DATE_REPORTED grouped by ZEST_KEY\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "def create_synthetic_date_of_request_simple(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    zest_key_col: str = \"ZEST_KEY\",\n",
    "    date_reported_col: str = \"DATE_REPORTED\",\n",
    "    zest_key_out: str = \"ZEST_KEY\",\n",
    "    rpt_col: str = \"rptDate\",\n",
    "    out_col: str = \"date_of_request\",\n",
    "    date_reported_format: str | None = \"%m%d%Y\",\n",
    "    seed: int | None = None,\n",
    "    verbose: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    ## apply zest ke and date fix \n",
    "    out = apply_converter_spec(\n",
    "        out,\n",
    "        {\"type\": \"StringConverterV2\", \"params\": {\"raw_feature\": zest_key_col, \"new_feature\": zest_key_out}},\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    date_spec: Dict[str, Any] = {\n",
    "        \"type\": \"DateConverterV2\",\n",
    "        \"params\": {\"raw_feature\": date_reported_col, \"new_feature\": rpt_col},\n",
    "    }\n",
    "    if date_reported_format:\n",
    "        date_spec[\"params\"][\"format\"] = date_reported_format\n",
    "    out = apply_converter_spec(out, date_spec, verbose=verbose)\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    # max rpt date\n",
    "    max_rpt = out.groupby(zest_key_out, dropna=False)[rpt_col].max()\n",
    "\n",
    "    ## mapping to random date for request\n",
    "    offsets = pd.Series(rng.integers(1, 4, size=len(max_rpt)), index=max_rpt.index)\n",
    "\n",
    "    dor_map = (max_rpt + offsets.map(lambda m: pd.DateOffset(months=int(m)))).rename(out_col)\n",
    "    out[out_col] = out[zest_key_out].map(dor_map)\n",
    "\n",
    "    return out\n",
    "def load_mapping_specs_from_json(path: Union[str, Path], *, key: str = \"mapping\") -> List[Dict[str, Any]]:\n",
    "    p = Path(path)\n",
    "    with p.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        obj = json.load(f)\n",
    "\n",
    "    if key not in obj:\n",
    "        raise KeyError(f\"JSON missing key '{key}'. Keys found: {list(obj.keys())}\")\n",
    "\n",
    "    specs = obj[key]\n",
    "    if not isinstance(specs, list):\n",
    "        raise TypeError(f\"json['{key}'] must be a list, got {type(specs)}\")\n",
    "\n",
    "    return specs\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# 3) Apply mappings from JSON + filter outputs\n",
    "#    (includes DATE_REPORTED as a feature to keep, per your request)\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "def apply_mapping_json_and_filter(\n",
    "    df: pd.DataFrame,\n",
    "    mapping_json_path: Union[str, Path],\n",
    "    *,\n",
    "    exclude_raw_features: Iterable[str] = (),\n",
    "    exclude_new_features: Iterable[str] = (),\n",
    "    extra_keep: Iterable[str] = (),\n",
    "    mapping_key: str = \"mapping\",\n",
    "    always_keep: Iterable[str] = (\"DATE_REPORTED\",),\n",
    "    strict: bool = False,\n",
    "    verbose: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    specs = load_mapping_specs_from_json(mapping_json_path, key=mapping_key)\n",
    "\n",
    "    exclude_raw = set(exclude_raw_features)\n",
    "    exclude_new = set(exclude_new_features)\n",
    "\n",
    "    def _spec_included(spec: Dict[str, Any]) -> bool:\n",
    "        params = spec.get(\"params\", {}) or {}\n",
    "        raw = params.get(\"raw_feature\")\n",
    "        new = params.get(\"new_feature\")\n",
    "        if raw and raw in exclude_raw:\n",
    "            return False\n",
    "        if new and new in exclude_new:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    specs_to_apply = [s for s in specs if _spec_included(s)]\n",
    "\n",
    "    out = df.copy()\n",
    "    for i, spec in enumerate(specs_to_apply, start=1):\n",
    "        if verbose:\n",
    "            print(f\"\\n[apply_mapping_json_and_filter] Step {i}/{len(specs_to_apply)}\")\n",
    "        out = apply_converter_spec(out, spec, verbose=verbose)\n",
    "\n",
    "    keep = build_output_feature_allowlist(specs_to_apply) | set(always_keep) | set(extra_keep)\n",
    "\n",
    "    missing = [c for c in keep if c not in out.columns]\n",
    "    if missing and strict:\n",
    "        raise KeyError(f\"Missing expected columns after mapping: {missing}\")\n",
    "\n",
    "    keep_in_df = [c for c in keep if c in out.columns]\n",
    "    return out.loc[:, keep_in_df].copy()\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List, Optional, Set, Union\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Preprocessor registry\n",
    "#    - Add/remove types based on what appears in your JSON preprocess section.\n",
    "# -----------------------------------------------------------------------------\n",
    "from feature_engine_parts.fe_parts_V2.preprocessors.date_diff import DateDiffV2\n",
    "from feature_engine_parts.fe_parts_V2.preprocessors.coalesce import CoalesceV2\n",
    "from feature_engine_parts.fe_parts_V2.preprocessors.bivariate_compute import BivariateComputeV2\n",
    "from feature_engine_parts.fe_parts_V2.preprocessors.dynamic_placeholder import DynamicPlaceholderV2\n",
    "from feature_engine_parts.fe_parts_V2.preprocessors.filter import FilterV2\n",
    "from feature_engine_parts.fe_parts_V2.preprocessors.payment_pattern_aggregator import PaymentPatternsAggregatorV2\n",
    "from feature_engine_parts.fe_parts_V2.preprocessors.row_dropper import RowDropperV2\n",
    "def build_preprocessor_registry():\n",
    "    return {\n",
    "        \"DateDiffV2\": DateDiffV2,\n",
    "        \"CoalesceV2\": CoalesceV2,\n",
    "        \"BivariateComputeV2\": BivariateComputeV2,\n",
    "        \"DynamicPlaceholderV2\": DynamicPlaceholderV2,\n",
    "        \"FilterV2\": FilterV2,\n",
    "        \"PaymentPatternsAggregatorV2\": PaymentPatternsAggregatorV2,\n",
    "        \"RowDropperV2\": RowDropperV2,  # <-- add this\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) JSON loading helpers\n",
    "# -----------------------------------------------------------------------------\n",
    "def load_specs_from_json(path: Union[str, Path], *, key: str) -> List[Dict[str, Any]]:\n",
    "    p = Path(path)\n",
    "    with p.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        obj = json.load(f)\n",
    "\n",
    "    if key not in obj:\n",
    "        raise KeyError(f\"JSON missing key '{key}'. Keys found: {list(obj.keys())}\")\n",
    "\n",
    "    specs = obj[key]\n",
    "    if not isinstance(specs, list):\n",
    "        raise TypeError(f\"json['{key}'] must be a list, got {type(specs)}\")\n",
    "\n",
    "    return specs\n",
    "\n",
    "\n",
    "def _output_feature_name_from_spec(spec: Dict[str, Any]) -> Optional[str]:\n",
    "    params = spec.get(\"params\", {}) or {}\n",
    "    # preprocessors often write to \"new_feature\"\n",
    "    return params.get(\"new_feature\") or params.get(\"raw_feature\")\n",
    "\n",
    "\n",
    "def build_output_feature_allowlist(specs: List[Dict[str, Any]]) -> Set[str]:\n",
    "    out: Set[str] = set()\n",
    "    for spec in specs:\n",
    "        name = _output_feature_name_from_spec(spec)\n",
    "        if name:\n",
    "            out.add(name)\n",
    "    return out\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Build + apply a single preprocessor spec\n",
    "# -----------------------------------------------------------------------------\n",
    "def _build_preprocessor(spec: Dict[str, Any], registry: Dict[str, Any]):\n",
    "    if not isinstance(spec, dict):\n",
    "        raise TypeError(f\"spec must be a dict, got {type(spec)}\")\n",
    "\n",
    "    type_name = spec.get(\"type\")\n",
    "    params = spec.get(\"params\", {}) or {}\n",
    "\n",
    "    if type_name not in registry:\n",
    "        known = \", \".join(sorted(registry.keys()))\n",
    "        raise ValueError(f\"Unknown preprocessor type '{type_name}'. Known types: {known}\")\n",
    "\n",
    "    if not isinstance(params, dict):\n",
    "        raise TypeError(f\"spec['params'] must be a dict, got {type(params)}\")\n",
    "\n",
    "    cls = registry[type_name]\n",
    "    return type_name, cls(**params)\n",
    "\n",
    "\n",
    "def apply_preprocess_spec(df: pd.DataFrame, spec: Dict[str, Any], *, registry: Dict[str, Any], verbose: bool = True) -> pd.DataFrame:\n",
    "    type_name, obj = _build_preprocessor(spec, registry)\n",
    "\n",
    "    if verbose:\n",
    "        params = spec.get(\"params\", {}) or {}\n",
    "        print(f\"[apply_preprocess_spec] {type_name} params={list(params.keys())}\")\n",
    "\n",
    "    return obj.transform(df)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Apply full preprocess section (the main function you want)\n",
    "# -----------------------------------------------------------------------------\n",
    "def apply_preprocess_json_and_filter(\n",
    "    mapped_df: pd.DataFrame,\n",
    "    mapping_json_path: Union[str, Path],\n",
    "    *,\n",
    "    preprocess_key: str = \"preprocess\",\n",
    "    # Keep behavior (optional): if you want to only return \"mapped cols + new preprocess outputs\"\n",
    "    keep_original_columns: bool = True,\n",
    "    extra_keep: Iterable[str] = (),\n",
    "    strict_keep: bool = False,\n",
    "    verbose: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply FE2 preprocess steps IN ORDER to an already-mapped df.\n",
    "\n",
    "    keep_original_columns=True:\n",
    "        returns (original mapped_df columns) + (preprocess-created outputs) + extra_keep\n",
    "\n",
    "    keep_original_columns=False:\n",
    "        returns only preprocess outputs + extra_keep\n",
    "    \"\"\"\n",
    "    registry = build_preprocessor_registry()\n",
    "    specs = load_specs_from_json(mapping_json_path, key=preprocess_key)\n",
    "\n",
    "    out = mapped_df.copy()\n",
    "    original_cols = list(out.columns)\n",
    "\n",
    "    for i, spec in enumerate(specs, start=1):\n",
    "        if verbose:\n",
    "            print(f\"\\n[apply_preprocess_json_and_filter] Step {i}/{len(specs)} | type={spec.get('type')}\")\n",
    "        out = apply_preprocess_spec(out, spec, registry=registry, verbose=verbose)\n",
    "\n",
    "    preprocess_outputs = build_output_feature_allowlist(specs)\n",
    "\n",
    "    if keep_original_columns:\n",
    "        keep = set(original_cols) | preprocess_outputs | set(extra_keep)\n",
    "    else:\n",
    "        keep = preprocess_outputs | set(extra_keep)\n",
    "\n",
    "    missing = [c for c in keep if c not in out.columns]\n",
    "    if missing and strict_keep:\n",
    "        raise KeyError(f\"Missing expected columns after preprocess: {missing}\")\n",
    "\n",
    "    keep_in_df = [c for c in keep if c in out.columns]\n",
    "    return out.loc[:, keep_in_df].copy()\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5) Convenience wrapper: mapping then preprocess (optional)\n",
    "# -----------------------------------------------------------------------------\n",
    "def apply_mapping_then_preprocess(\n",
    "    df_raw: pd.DataFrame,\n",
    "    mapping_json_path: Union[str, Path],\n",
    "    *,\n",
    "    mapping_func,  # pass your existing apply_mapping_json_and_filter\n",
    "    mapping_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    preprocess_kwargs: Optional[Dict[str, Any]] = None,\n",
    ") -> pd.DataFrame:\n",
    "    mapping_kwargs = mapping_kwargs or {}\n",
    "    preprocess_kwargs = preprocess_kwargs or {}\n",
    "\n",
    "    mapped = mapping_func(df=df_raw, mapping_json_path=mapping_json_path, **mapping_kwargs)\n",
    "    preprocessed = apply_preprocess_json_and_filter(mapped, mapping_json_path, **preprocess_kwargs)\n",
    "    return preprocessed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5392a9a-2fda-4903-9098-ed6b71b09a43",
   "metadata": {},
   "source": [
    "# Create S3 Handler Object and Load Parsed and Processed for inquiry trade collection bankruptcy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "659bb4cc-0fe0-4b0d-aa35-1fcf120d86a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jag/.conda/envs/newest_model_engine/lib/python3.10/site-packages/zaml/common/utils/io.py:17: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are loading for parsed files\n",
      "Loading for inquiry\n",
      "Num of files is 29\n",
      "Loading for trade\n",
      "Num of files is 23\n",
      "Loading for collection\n",
      "Num of files is 1\n",
      "Loading for bankruptcy\n",
      "Num of files is 1\n",
      "we are loading processed files\n",
      "Loading for inquiry\n",
      "Num of files is 1\n",
      "Loading for trade\n",
      "Num of files is 1\n",
      "Loading for collection\n",
      "Num of files is 1\n",
      "Loading for bankruptcy\n",
      "Num of files is 1\n",
      "Loading for target\n",
      "Num of files is 1\n"
     ]
    }
   ],
   "source": [
    "from zestio import handler\n",
    "import pandas as pd\n",
    "bucket_name=\"power-client-data-staging\"\n",
    "s3 = handler.S3Handler(bucket_name=bucket_name)\n",
    "archive_date = 'ARCHIVE_DATE=2025-03-31'\n",
    "def grab_parsed_data(s3, archive_date = archive_date, PULL_NAME='PULL_NAME=20260118_atlanticfcu_consolidatedfcu_heartlandcu_midminnesotafcu_sanfranciscofcu_vantagewestcu'):\n",
    "    return_dict = {}\n",
    "    print(f'We are loading for parsed files')\n",
    "    for table in ['inquiry', 'trade', 'collection', 'bankruptcy']:\n",
    "        print(f'Loading for {table}')\n",
    "        prefix = f'CLIENT/PARSED/DATA/BUREAU=equifax/FORMAT=cms_6/TABLE={table}/{PULL_NAME}/{archive_date}'\n",
    "        file_list = s3.list(prefix, recursive = False)\n",
    "        return_dict[table] = {'file_list': file_list, 'prefix': prefix}\n",
    "        print(f'Num of files is {len(file_list)}')\n",
    "    return return_dict\n",
    "def grab_processed_data_atlantic(s3, archive_date = archive_date):\n",
    "    return_dict = {}\n",
    "    print('we are loading processed files')\n",
    "    for table in ['inquiry', 'trade', 'collection', 'bankruptcy', 'target']:\n",
    "        print(f'Loading for {table}')\n",
    "        prefix = f'PROCESSED/DATA/TABLE={table}/VERSION=v2/CLIENT=atlanticfcu/PRODUCT=autoloanv2/BUREAU=equifax/FORMAT=cms_6/PULL_DATE=2026-01-18/PULL_NAME=20260118_atlanticfcu_consolidatedfcu_heartlandcu_midminnesotafcu_sanfranciscofcu_vantagewestcu/ME_VERSION=v2.0.1/{archive_date}/'\n",
    "        file_list = s3.list(prefix, recursive = False)\n",
    "        print(f'Num of files is {len(file_list)}')\n",
    "        return_dict[table] = s3._read_parquet_dataset(prefix)\n",
    "    return return_dict\n",
    "dict_of_parsed_files = grab_parsed_data(s3)\n",
    "dict_of_processed_data = grab_processed_data_atlantic(s3)\n",
    "\n",
    "## grab all the IDs for atlatnic \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60b24146-3f0f-4cff-8d75-2af3cc84b74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = dict_of_processed_data['target'].reset_index()\n",
    "zest_key_df = target_df[['ZEST_KEY', 'appDate']].rename(columns = {'appDate': 'date_of_request'})\n",
    "\n",
    "trade_files =  dict_of_parsed_files['trade']['file_list']\n",
    "trade_prefix = dict_of_parsed_files['trade']['prefix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91f49a58-8e92-4bb9-8d46-fc6483a6e6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute intensive \n",
    "import warnings\n",
    "\n",
    "# This way loads all at once using s3. Could be compute intensive\n",
    "#current_trade_file_df = s3._read_parquet_dataset(trade_prefix)\n",
    "trade_df_list = []\n",
    "for file in trade_files:\n",
    "    location = f'{trade_prefix}/{file}'\n",
    "    current_trade_file_df = pd.read_parquet(f's3://{bucket_name}/{location}')\n",
    "    trade_df_list.append(zest_key_df.merge(current_trade_file_df, on = ['ZEST_KEY'], how = 'inner'))\n",
    "current_trade_file_df = pd.concat(trade_df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3f6212-efdc-45b0-9736-0557e9d22ad0",
   "metadata": {},
   "source": [
    "# Prepare the Tradeline Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6bab2f00-a8bb-4ac8-9862-8e6b1ed0a28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[apply_mapping_json_and_filter] Step 1/29\n",
      "[apply_converter_spec] Using StringConverterV2 | raw_feature='ZEST_KEY' -> new_feature='ZEST_KEY'\n",
      "\n",
      "[apply_mapping_json_and_filter] Step 2/29\n",
      "[apply_converter_spec] Using NumericConverterV2 | raw_feature='BALANCE' -> new_feature='balance_amt'\n",
      "\n",
      "[apply_mapping_json_and_filter] Step 3/29\n",
      "[apply_converter_spec] Using NumericConverterV2 | raw_feature='CREDIT_LIMIT' -> new_feature='credit_limit'\n",
      "\n",
      "[apply_mapping_json_and_filter] Step 4/29\n",
      "[apply_converter_spec] Using NumericConverterV2 | raw_feature='HIGH_CREDIT' -> new_feature='high_credit_amt'\n",
      "\n",
      "[apply_mapping_json_and_filter] Step 5/29\n",
      "[apply_converter_spec] Using NumericConverterV2 | raw_feature='PAST_DUE_AMOUNT' -> new_feature='pastDueAmt'\n",
      "\n",
      "[apply_mapping_json_and_filter] Step 6/29\n",
      "[apply_converter_spec] Using NumericConverterV2 | raw_feature='SCHEDULED_PAYMENT_AMOUNT' -> new_feature='scheduled_payment_amount'\n",
      "\n",
      "[apply_mapping_json_and_filter] Step 7/29\n",
      "[apply_converter_spec] Using DateConverterV2 | raw_feature='DATE_OPENED' -> new_feature='openDate'\n",
      "\n",
      "[apply_mapping_json_and_filter] Step 8/29\n",
      "[apply_converter_spec] Using DateConverterV2 | raw_feature='CLOSED_DATE' -> new_feature='closedDate'\n",
      "\n",
      "[apply_mapping_json_and_filter] Step 9/29\n",
      "[apply_converter_spec] Using DateConverterV2 | raw_feature='DATE_REPORTED' -> new_feature='rptDate'\n",
      "\n",
      "[apply_mapping_json_and_filter] Step 10/29\n",
      "[apply_converter_spec] Using DateConverterV2 | raw_feature='DMD_REPORTED' -> new_feature='majordqDate'\n",
      "\n",
      "[apply_mapping_json_and_filter] Step 11/29\n",
      "[apply_converter_spec] Using DateConverterV2 | raw_feature='LAST_PAYMENT_DATE' -> new_feature='lstPmtDate'\n",
      "\n",
      "[apply_mapping_json_and_filter] Step 12/29\n",
      "[apply_converter_spec] Using DateConverterV2 | raw_feature='PREVIOUS_HIGH_DATE_1' -> new_feature='dqDate'\n",
      "\n",
      "[apply_mapping_json_and_filter] Step 13/29\n",
      "[apply_converter_spec] Using NumericConverterV2 | raw_feature='TERMS_FREQUENCY' -> new_feature='termFreqMult'\n",
      "\n",
      "[apply_mapping_json_and_filter] Step 14/29\n",
      "[apply_converter_spec] Using StringConverterV2 | raw_feature='TERMS_FREQUENCY' -> new_feature='termFreqStr'\n",
      "\n",
      "[apply_mapping_json_and_filter] Step 15/29\n",
      "[apply_converter_spec] Using NumericConverterV2 | raw_feature='TERMS_DURATION' -> new_feature='termDur'\n",
      "\n",
      "[apply_mapping_json_and_filter] Step 16/29\n",
      "[apply_converter_spec] Using StringConverterV2 | raw_feature='PAYMENT_HISTORY_1_24' -> new_feature='PAYMENT_HISTORY_1_24'\n",
      "\n",
      "[apply_mapping_json_and_filter] Step 17/29\n",
      "[apply_converter_spec] Using StringConverterV2 | raw_feature='PAYMENT_HISTORY_25_36' -> new_feature='PAYMENT_HISTORY_25_36'\n",
      "\n",
      "[apply_mapping_json_and_filter] Step 18/29\n",
      "[apply_converter_spec] Using StringConverterV2 | raw_feature='PAYMENT_HISTORY_37_48' -> new_feature='PAYMENT_HISTORY_37_48'\n",
      "\n",
      "[apply_mapping_json_and_filter] Step 19/29\n",
      "[apply_converter_spec] Using StringConverterV2 | raw_feature='RATE_STATUS_CODE' -> new_feature='RATE_STATUS_CODE'\n",
      "\n",
      "[apply_mapping_json_and_filter] Step 20/29\n",
      "[apply_converter_spec] Using StringConverterV2 | raw_feature='ACCOUNT_TYPE' -> new_feature='accountType'\n",
      "\n",
      "[apply_mapping_json_and_filter] Step 21/29\n",
      "[apply_converter_spec] Using StringConverterV2 | raw_feature='PORTFOLIO_TYPE' -> new_feature='portfolioType'\n",
      "\n",
      "[apply_mapping_json_and_filter] Step 22/29\n",
      "[apply_converter_spec] Using StringConverterV2 | raw_feature='PORTFOLIO_TYPE' -> new_feature='PORTFOLIO_TYPE'\n",
      "\n",
      "[apply_mapping_json_and_filter] Step 23/29\n",
      "[apply_converter_spec] Using StringConverterV2 | raw_feature='ECOA_DESIGNATOR' -> new_feature='ecoa'\n",
      "\n",
      "[apply_mapping_json_and_filter] Step 24/29\n",
      "[apply_converter_spec] Using StringConverterV2 | raw_feature='NARRATIVE_CODE_1' -> new_feature='NARRATIVE_CODE_1'\n",
      "\n",
      "[apply_mapping_json_and_filter] Step 25/29\n",
      "[apply_converter_spec] Using StringConverterV2 | raw_feature='NARRATIVE_CODE_2' -> new_feature='NARRATIVE_CODE_2'\n",
      "\n",
      "[apply_mapping_json_and_filter] Step 26/29\n",
      "[apply_converter_spec] Using StringConverterV2 | raw_feature='ACTIVITY_DESIGNATOR' -> new_feature='ACTIVITY_DESIGNATOR'\n",
      "\n",
      "[apply_mapping_json_and_filter] Step 27/29\n",
      "[apply_converter_spec] Using NumericConverterV2 | raw_feature='ORIGINAL_CHARGE_OFF_AMOUNT' -> new_feature='chargeoff_amt'\n",
      "\n",
      "[apply_mapping_json_and_filter] Step 28/29\n",
      "[apply_converter_spec] Using StringConverterV2 | raw_feature='CUSTOMER_NUMBER' -> new_feature='CUSTOMER_NUMBER'\n",
      "\n",
      "[apply_mapping_json_and_filter] Step 29/29\n",
      "[apply_converter_spec] Using StringConverterV2 | raw_feature='ZEST_KEY' -> new_feature='constant_placeholder'\n"
     ]
    }
   ],
   "source": [
    "trade_df_first_part_of_processing = apply_mapping_json_and_filter(\n",
    "    df=current_trade_file_df,\n",
    "    mapping_json_path=\"../model-engine/model_engine/assets/equifax/cms_6/fe2/trade.json\",\n",
    "    exclude_raw_features={\"DATE_OF_REQUEST\"},\n",
    "    exclude_new_features={\"date_of_request\"},\n",
    "    extra_keep={\"DATE_OF_REQUEST\", \"DMD_REPORTED\", 'date_of_request'},\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1d9cf48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dqDate</th>\n",
       "      <th>ZEST_KEY</th>\n",
       "      <th>pastDueAmt</th>\n",
       "      <th>scheduled_payment_amount</th>\n",
       "      <th>date_of_request</th>\n",
       "      <th>chargeoff_amt</th>\n",
       "      <th>lstPmtDate</th>\n",
       "      <th>ecoa</th>\n",
       "      <th>NARRATIVE_CODE_2</th>\n",
       "      <th>termFreqMult</th>\n",
       "      <th>...</th>\n",
       "      <th>PAYMENT_HISTORY_37_48</th>\n",
       "      <th>termFreqStr</th>\n",
       "      <th>PAYMENT_HISTORY_25_36</th>\n",
       "      <th>termDur</th>\n",
       "      <th>closedDate</th>\n",
       "      <th>credit_limit</th>\n",
       "      <th>NARRATIVE_CODE_1</th>\n",
       "      <th>PAYMENT_HISTORY_1_24</th>\n",
       "      <th>openDate</th>\n",
       "      <th>DMD_REPORTED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaT</td>\n",
       "      <td>76159_1_087_02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2025-04-17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-02-01</td>\n",
       "      <td>I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>/111111EE111E</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/111111111111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1500.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>111111111111/111111111111</td>\n",
       "      <td>2017-06-12</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaT</td>\n",
       "      <td>76159_1_087_02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2025-04-17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-02-01</td>\n",
       "      <td>I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>/11111111111*</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/111111111111</td>\n",
       "      <td>48.00</td>\n",
       "      <td>2023-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EP</td>\n",
       "      <td>************/***********1</td>\n",
       "      <td>2020-10-14</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaT</td>\n",
       "      <td>76159_1_087_02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2025-04-17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>J</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>/************</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/************</td>\n",
       "      <td>36.00</td>\n",
       "      <td>2017-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>************/************</td>\n",
       "      <td>2015-07-18</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaT</td>\n",
       "      <td>76159_1_087_02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2025-04-17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>/************</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/************</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2007-12-01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>IR</td>\n",
       "      <td>************/************</td>\n",
       "      <td>2007-11-23</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaT</td>\n",
       "      <td>76392_1_087_02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2025-05-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-02-01</td>\n",
       "      <td>I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>/1111111EE111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/11111111111E</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1280.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>111EEEEEEEEE/E1111E1111E1</td>\n",
       "      <td>2013-09-01</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>NaT</td>\n",
       "      <td>77308_1_087_02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>2025-06-23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-03-01</td>\n",
       "      <td>I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>/************</td>\n",
       "      <td>M</td>\n",
       "      <td>/************</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>500.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1111111EE***/************</td>\n",
       "      <td>2024-05-25</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>NaT</td>\n",
       "      <td>77308_1_087_02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>44.00</td>\n",
       "      <td>2025-06-23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-03-01</td>\n",
       "      <td>I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>/11111111****</td>\n",
       "      <td>M</td>\n",
       "      <td>/111111111111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1600.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>111111111111/111111111111</td>\n",
       "      <td>2021-06-02</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>2019-09-01</td>\n",
       "      <td>77308_1_087_02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>2025-06-23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-02-01</td>\n",
       "      <td>I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>/111111111111</td>\n",
       "      <td>M</td>\n",
       "      <td>/111111111111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>550.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>111111111111/111111111111</td>\n",
       "      <td>2019-04-21</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>NaT</td>\n",
       "      <td>77308_1_087_02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2025-06-23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-02-01</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>/111111111111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/111111111111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>6300.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>111111111111/111111111111</td>\n",
       "      <td>2010-01-24</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>NaT</td>\n",
       "      <td>77308_1_087_02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>2025-06-23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-02-01</td>\n",
       "      <td>I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>/************</td>\n",
       "      <td>M</td>\n",
       "      <td>/************</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1300.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>111111111111/1E1111******</td>\n",
       "      <td>2023-07-06</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3091 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        dqDate        ZEST_KEY  pastDueAmt  scheduled_payment_amount  \\\n",
       "0          NaT  76159_1_087_02        0.00                      0.00   \n",
       "1          NaT  76159_1_087_02        0.00                      0.00   \n",
       "2          NaT  76159_1_087_02        0.00                      0.00   \n",
       "3          NaT  76159_1_087_02        0.00                      0.00   \n",
       "4          NaT  76392_1_087_02        0.00                      0.00   \n",
       "..         ...             ...         ...                       ...   \n",
       "238        NaT  77308_1_087_02        0.00                     25.00   \n",
       "239        NaT  77308_1_087_02        0.00                     44.00   \n",
       "240 2019-09-01  77308_1_087_02        0.00                     30.00   \n",
       "241        NaT  77308_1_087_02        0.00                      0.00   \n",
       "242        NaT  77308_1_087_02        0.00                     25.00   \n",
       "\n",
       "    date_of_request  chargeoff_amt lstPmtDate ecoa NARRATIVE_CODE_2  \\\n",
       "0        2025-04-17            NaN 2025-02-01    I              NaN   \n",
       "1        2025-04-17            NaN 2023-02-01    I              NaN   \n",
       "2        2025-04-17            NaN 2017-01-01    J              NaN   \n",
       "3        2025-04-17            NaN        NaT    I              NaN   \n",
       "4        2025-05-01            NaN 2025-02-01    I              NaN   \n",
       "..              ...            ...        ...  ...              ...   \n",
       "238      2025-06-23            NaN 2025-03-01    I              NaN   \n",
       "239      2025-06-23            NaN 2025-03-01    I              NaN   \n",
       "240      2025-06-23            NaN 2025-02-01    I              NaN   \n",
       "241      2025-06-23            NaN 2025-02-01    A              NaN   \n",
       "242      2025-06-23            NaN 2025-02-01    I              NaN   \n",
       "\n",
       "     termFreqMult  ...  PAYMENT_HISTORY_37_48 termFreqStr  \\\n",
       "0            1.00  ...          /111111EE111E         NaN   \n",
       "1            1.00  ...          /11111111111*         NaN   \n",
       "2            1.00  ...          /************         NaN   \n",
       "3            1.00  ...          /************         NaN   \n",
       "4            1.00  ...          /1111111EE111         NaN   \n",
       "..            ...  ...                    ...         ...   \n",
       "238          1.00  ...          /************           M   \n",
       "239          1.00  ...          /11111111****           M   \n",
       "240          1.00  ...          /111111111111           M   \n",
       "241          1.00  ...          /111111111111         NaN   \n",
       "242          1.00  ...          /************           M   \n",
       "\n",
       "    PAYMENT_HISTORY_25_36 termDur closedDate credit_limit NARRATIVE_CODE_1  \\\n",
       "0           /111111111111     NaN        NaT      1500.00              NaN   \n",
       "1           /111111111111   48.00 2023-02-01          NaN               EP   \n",
       "2           /************   36.00 2017-03-01          NaN              NaN   \n",
       "3           /************     NaN 2007-12-01         0.00               IR   \n",
       "4           /11111111111E     NaN        NaT      1280.00              NaN   \n",
       "..                    ...     ...        ...          ...              ...   \n",
       "238         /************     NaN        NaT       500.00              NaN   \n",
       "239         /111111111111     NaN        NaT      1600.00              NaN   \n",
       "240         /111111111111     NaN        NaT       550.00              NaN   \n",
       "241         /111111111111     NaN        NaT      6300.00              NaN   \n",
       "242         /************     NaN        NaT      1300.00              NaN   \n",
       "\n",
       "          PAYMENT_HISTORY_1_24   openDate DMD_REPORTED  \n",
       "0    111111111111/111111111111 2017-06-12         None  \n",
       "1    ************/***********1 2020-10-14         None  \n",
       "2    ************/************ 2015-07-18         None  \n",
       "3    ************/************ 2007-11-23         None  \n",
       "4    111EEEEEEEEE/E1111E1111E1 2013-09-01         None  \n",
       "..                         ...        ...          ...  \n",
       "238  1111111EE***/************ 2024-05-25         None  \n",
       "239  111111111111/111111111111 2021-06-02         None  \n",
       "240  111111111111/111111111111 2019-04-21         None  \n",
       "241  111111111111/111111111111 2010-01-24         None  \n",
       "242  111111111111/1E1111****** 2023-07-06         None  \n",
       "\n",
       "[3091 rows x 32 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trade_df_first_part_of_processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de3d5dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[apply_preprocess_json_and_filter] Step 1/100 | type=DateDiffV2\n",
      "[apply_preprocess_spec] DateDiffV2 params=['feature', 'reference_feature', 'new_feature']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 2/100 | type=DateDiffV2\n",
      "[apply_preprocess_spec] DateDiffV2 params=['feature', 'reference_feature', 'new_feature']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 3/100 | type=DateDiffV2\n",
      "[apply_preprocess_spec] DateDiffV2 params=['feature', 'reference_feature', 'new_feature']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 4/100 | type=PaymentPatternsAggregatorV2\n",
      "[apply_preprocess_spec] PaymentPatternsAggregatorV2 params=['report_date', 'payment_patterns']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 5/100 | type=CoalesceV2\n",
      "[apply_preprocess_spec] CoalesceV2 params=['feature_1', 'feature_2', 'new_feature', 'mode', 'input_dtype']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 6/100 | type=DynamicPlaceholderV2\n",
      "[apply_preprocess_spec] DynamicPlaceholderV2 params=['raw_feature', 'new_value', 'filter_params', 'raw_feature_input_dtype']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 7/100 | type=DynamicPlaceholderV2\n",
      "[apply_preprocess_spec] DynamicPlaceholderV2 params=['raw_feature', 'new_value', 'filter_params', 'raw_feature_input_dtype']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 8/100 | type=DynamicPlaceholderV2\n",
      "[apply_preprocess_spec] DynamicPlaceholderV2 params=['raw_feature', 'new_value', 'raw_feature_input_dtype', 'filter_params']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 9/100 | type=BivariateComputeV2\n",
      "[apply_preprocess_spec] BivariateComputeV2 params=['feature_1', 'feature_2', 'op', 'new_feature']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 10/100 | type=BivariateComputeV2\n",
      "[apply_preprocess_spec] BivariateComputeV2 params=['feature_1', 'feature_2', 'op', 'new_feature']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 11/100 | type=BivariateComputeV2\n",
      "[apply_preprocess_spec] BivariateComputeV2 params=['feature_1', 'feature_2', 'op', 'new_feature']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 12/100 | type=BivariateComputeV2\n",
      "[apply_preprocess_spec] BivariateComputeV2 params=['feature_1', 'feature_2', 'op', 'new_feature']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 13/100 | type=BivariateComputeV2\n",
      "[apply_preprocess_spec] BivariateComputeV2 params=['feature_1', 'feature_2', 'op', 'new_feature']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 14/100 | type=BivariateComputeV2\n",
      "[apply_preprocess_spec] BivariateComputeV2 params=['feature_1', 'feature_2', 'op', 'new_feature']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 15/100 | type=BivariateComputeV2\n",
      "[apply_preprocess_spec] BivariateComputeV2 params=['feature_1', 'feature_2', 'op', 'new_feature']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 16/100 | type=BivariateComputeV2\n",
      "[apply_preprocess_spec] BivariateComputeV2 params=['feature_1', 'feature_2', 'op', 'new_feature']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 17/100 | type=BivariateComputeV2\n",
      "[apply_preprocess_spec] BivariateComputeV2 params=['feature_1', 'feature_2', 'op', 'new_feature']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 18/100 | type=BivariateComputeV2\n",
      "[apply_preprocess_spec] BivariateComputeV2 params=['feature_1', 'feature_2', 'op', 'new_feature']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 19/100 | type=DynamicPlaceholderV2\n",
      "[apply_preprocess_spec] DynamicPlaceholderV2 params=['raw_feature', 'new_value', 'filter_params', 'raw_feature_input_dtype']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 20/100 | type=BivariateComputeV2\n",
      "[apply_preprocess_spec] BivariateComputeV2 params=['feature_1', 'feature_2', 'op', 'new_feature']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 21/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 22/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 23/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 24/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 25/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 26/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 27/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 28/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 29/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 30/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 31/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 32/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 33/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 34/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 35/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 36/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 37/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 38/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 39/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 40/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 41/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 42/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 43/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 44/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 45/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 46/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 47/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 48/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 49/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 50/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 51/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 52/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 53/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 54/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 55/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 56/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 57/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 58/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 59/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 60/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 61/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 62/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 63/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 64/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 65/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 66/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 67/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 68/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 69/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 70/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 71/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 72/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 73/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 74/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 75/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 76/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 77/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 78/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 79/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 80/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 81/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 82/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 83/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 84/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 85/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 86/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 87/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 88/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 89/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 90/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 91/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 92/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 93/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 94/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 95/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 96/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 97/100 | type=FilterV2\n",
      "[apply_preprocess_spec] FilterV2 params=['new_feature', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 98/100 | type=RowDropperV2\n",
      "[apply_preprocess_spec] RowDropperV2 params=['transformer_name', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 99/100 | type=RowDropperV2\n",
      "[apply_preprocess_spec] RowDropperV2 params=['transformer_name', 'conditions', 'outer_op']\n",
      "\n",
      "[apply_preprocess_json_and_filter] Step 100/100 | type=RowDropperV2\n",
      "[apply_preprocess_spec] RowDropperV2 params=['transformer_name', 'conditions', 'outer_op']\n"
     ]
    }
   ],
   "source": [
    "preprocessed_df = apply_preprocess_json_and_filter(\n",
    "    mapped_df=trade_df_first_part_of_processing,\n",
    "    mapping_json_path=\"../model-engine/model_engine/assets/equifax/cms_6/fe2/trade.json\",\n",
    "    keep_original_columns=True,\n",
    "    extra_keep={\"DATE_OF_REQUEST\", \"rptDate\", \"DMD_REPORTED\", \"date_of_request\", 'closedDate', 'majordqDate', 'zest_payment_pattern',   \"PAYMENT_HISTORY_1_24\",\n",
    "                        \"PAYMENT_HISTORY_25_36\",\n",
    "                        \"PAYMENT_HISTORY_37_48\"},\n",
    "    strict_keep=False,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faa5f54-2479-4585-86f8-5ee6de1fc320",
   "metadata": {},
   "source": [
    "# We use the most recent data pull for the target!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "6b9f6dd7-1e31-4a36-b6e9-e2c6df66b0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df['app_month'] = target_df['appDate'].dt.to_period('M')\n",
    "\n",
    "# Get all unique months (sorted)\n",
    "all_months = (\n",
    "    target_df['app_month']\n",
    "    .dropna()\n",
    "    .sort_values()\n",
    "    .unique()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "60ac8476-fbb4-43c1-a82d-284bc883e080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that we have 3 months of app dates per target\n",
    "\n",
    "# bascially archive date 2024-12-31 means their application date was between "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8483f15d-44d3-4522-99d8-fbb07df2ac12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PeriodArray>\n",
       "['2025-04', '2025-05', '2025-06']\n",
       "Length: 3, dtype: period[M]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "7d48ee42-7619-44f3-820b-8f3a6a73e720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "final_DQ30_m9_status\n",
       "unseason    103\n",
       "nohit        79\n",
       "season        4\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_df['final_DQ30_m9_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "6c3591fa-e320-44c7-906e-3060940f1fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "app_month  final_DQ30_m9_status\n",
       "2025-03    unseason                42\n",
       "2025-01    unseason                34\n",
       "           nohit                   33\n",
       "2025-02    unseason                27\n",
       "2025-03    nohit                   27\n",
       "2025-02    nohit                   20\n",
       "2025-01    season                  15\n",
       "2025-02    season                  15\n",
       "2025-03    season                   5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_df[['app_month', 'final_DQ30_m9_status']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "264ddfc9-e479-45d8-ab40-d51a21df136c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('<M8[ns]')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_df['appDate'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "89c4653c-639c-4a21-8696-d33b5e01864e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>appId</th>\n",
       "      <th>appDate</th>\n",
       "      <th>flgFunded</th>\n",
       "      <th>flgApproved</th>\n",
       "      <th>final_DQ30_m3</th>\n",
       "      <th>final_DQ30_m3_source</th>\n",
       "      <th>final_DQ30_m3_status</th>\n",
       "      <th>final_DQ30_m3_sample_weight</th>\n",
       "      <th>final_DateDQ30_m3</th>\n",
       "      <th>final_DQ30_m6</th>\n",
       "      <th>...</th>\n",
       "      <th>final_DateCO_m30</th>\n",
       "      <th>final_CO_m36</th>\n",
       "      <th>final_CO_m36_source</th>\n",
       "      <th>final_CO_m36_status</th>\n",
       "      <th>final_CO_m36_sample_weight</th>\n",
       "      <th>final_DateCO_m36</th>\n",
       "      <th>proxy_MSO_target_CO</th>\n",
       "      <th>proxy_CO</th>\n",
       "      <th>final_MSO_target_CO</th>\n",
       "      <th>final_CO</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZEST_KEY</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76745_1_087_02</th>\n",
       "      <td>76745</td>\n",
       "      <td>2025-05-22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>missing</td>\n",
       "      <td>unseason</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>missing</td>\n",
       "      <td>unseason</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76872_1_087_02</th>\n",
       "      <td>76872</td>\n",
       "      <td>2025-05-30</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>missing</td>\n",
       "      <td>unseason</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>missing</td>\n",
       "      <td>unseason</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77113_1_087_02</th>\n",
       "      <td>77113</td>\n",
       "      <td>2025-06-11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>missing</td>\n",
       "      <td>unseason</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>missing</td>\n",
       "      <td>unseason</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77208_1_087_02</th>\n",
       "      <td>77208</td>\n",
       "      <td>2025-06-16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>missing</td>\n",
       "      <td>unseason</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>missing</td>\n",
       "      <td>unseason</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77208_2_087_02</th>\n",
       "      <td>77208</td>\n",
       "      <td>2025-06-16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>missing</td>\n",
       "      <td>unseason</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>missing</td>\n",
       "      <td>unseason</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77229_1_087_02</th>\n",
       "      <td>77229</td>\n",
       "      <td>2025-06-17</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>missing</td>\n",
       "      <td>unseason</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>missing</td>\n",
       "      <td>unseason</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77229_2_087_02</th>\n",
       "      <td>77229</td>\n",
       "      <td>2025-06-17</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>missing</td>\n",
       "      <td>unseason</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>missing</td>\n",
       "      <td>unseason</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77243_2_087_02</th>\n",
       "      <td>77243</td>\n",
       "      <td>2025-06-18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>missing</td>\n",
       "      <td>unseason</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>missing</td>\n",
       "      <td>unseason</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77243_1_087_02</th>\n",
       "      <td>77243</td>\n",
       "      <td>2025-06-18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>missing</td>\n",
       "      <td>unseason</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>missing</td>\n",
       "      <td>unseason</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77320_1_087_02</th>\n",
       "      <td>77320</td>\n",
       "      <td>2025-06-23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>missing</td>\n",
       "      <td>unseason</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>missing</td>\n",
       "      <td>unseason</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77390_1_087_02</th>\n",
       "      <td>77390</td>\n",
       "      <td>2025-06-27</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>missing</td>\n",
       "      <td>unseason</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>missing</td>\n",
       "      <td>unseason</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77411_1_087_02</th>\n",
       "      <td>77411</td>\n",
       "      <td>2025-06-28</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>missing</td>\n",
       "      <td>unseason</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>missing</td>\n",
       "      <td>unseason</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77421_2_087_02</th>\n",
       "      <td>77421</td>\n",
       "      <td>2025-06-30</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>missing</td>\n",
       "      <td>unseason</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>missing</td>\n",
       "      <td>unseason</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77421_1_087_02</th>\n",
       "      <td>77421</td>\n",
       "      <td>2025-06-30</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>missing</td>\n",
       "      <td>unseason</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>missing</td>\n",
       "      <td>unseason</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14 rows Ã— 188 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                appId    appDate  flgFunded  flgApproved  final_DQ30_m3  \\\n",
       "ZEST_KEY                                                                  \n",
       "76745_1_087_02  76745 2025-05-22          0            0            NaN   \n",
       "76872_1_087_02  76872 2025-05-30          1            1            NaN   \n",
       "77113_1_087_02  77113 2025-06-11          0            0            NaN   \n",
       "77208_1_087_02  77208 2025-06-16          1            1            NaN   \n",
       "77208_2_087_02  77208 2025-06-16          1            1            NaN   \n",
       "77229_1_087_02  77229 2025-06-17          1            1            NaN   \n",
       "77229_2_087_02  77229 2025-06-17          1            1            NaN   \n",
       "77243_2_087_02  77243 2025-06-18          1            1            NaN   \n",
       "77243_1_087_02  77243 2025-06-18          1            1            NaN   \n",
       "77320_1_087_02  77320 2025-06-23          0            0            NaN   \n",
       "77390_1_087_02  77390 2025-06-27          1            1            NaN   \n",
       "77411_1_087_02  77411 2025-06-28          1            1            NaN   \n",
       "77421_2_087_02  77421 2025-06-30          1            1            NaN   \n",
       "77421_1_087_02  77421 2025-06-30          1            1            NaN   \n",
       "\n",
       "               final_DQ30_m3_source final_DQ30_m3_status  \\\n",
       "ZEST_KEY                                                   \n",
       "76745_1_087_02              missing             unseason   \n",
       "76872_1_087_02              missing             unseason   \n",
       "77113_1_087_02              missing             unseason   \n",
       "77208_1_087_02              missing             unseason   \n",
       "77208_2_087_02              missing             unseason   \n",
       "77229_1_087_02              missing             unseason   \n",
       "77229_2_087_02              missing             unseason   \n",
       "77243_2_087_02              missing             unseason   \n",
       "77243_1_087_02              missing             unseason   \n",
       "77320_1_087_02              missing             unseason   \n",
       "77390_1_087_02              missing             unseason   \n",
       "77411_1_087_02              missing             unseason   \n",
       "77421_2_087_02              missing             unseason   \n",
       "77421_1_087_02              missing             unseason   \n",
       "\n",
       "                final_DQ30_m3_sample_weight final_DateDQ30_m3  final_DQ30_m6  \\\n",
       "ZEST_KEY                                                                       \n",
       "76745_1_087_02                            1               NaT            NaN   \n",
       "76872_1_087_02                            1               NaT            NaN   \n",
       "77113_1_087_02                            1               NaT            NaN   \n",
       "77208_1_087_02                            1               NaT            NaN   \n",
       "77208_2_087_02                            1               NaT            NaN   \n",
       "77229_1_087_02                            1               NaT            NaN   \n",
       "77229_2_087_02                            1               NaT            NaN   \n",
       "77243_2_087_02                            1               NaT            NaN   \n",
       "77243_1_087_02                            1               NaT            NaN   \n",
       "77320_1_087_02                            1               NaT            NaN   \n",
       "77390_1_087_02                            1               NaT            NaN   \n",
       "77411_1_087_02                            1               NaT            NaN   \n",
       "77421_2_087_02                            1               NaT            NaN   \n",
       "77421_1_087_02                            1               NaT            NaN   \n",
       "\n",
       "                ... final_DateCO_m30 final_CO_m36  final_CO_m36_source  \\\n",
       "ZEST_KEY        ...                                                      \n",
       "76745_1_087_02  ...              NaT          NaN              missing   \n",
       "76872_1_087_02  ...              NaT          NaN              missing   \n",
       "77113_1_087_02  ...              NaT          NaN              missing   \n",
       "77208_1_087_02  ...              NaT          NaN              missing   \n",
       "77208_2_087_02  ...              NaT          NaN              missing   \n",
       "77229_1_087_02  ...              NaT          NaN              missing   \n",
       "77229_2_087_02  ...              NaT          NaN              missing   \n",
       "77243_2_087_02  ...              NaT          NaN              missing   \n",
       "77243_1_087_02  ...              NaT          NaN              missing   \n",
       "77320_1_087_02  ...              NaT          NaN              missing   \n",
       "77390_1_087_02  ...              NaT          NaN              missing   \n",
       "77411_1_087_02  ...              NaT          NaN              missing   \n",
       "77421_2_087_02  ...              NaT          NaN              missing   \n",
       "77421_1_087_02  ...              NaT          NaN              missing   \n",
       "\n",
       "               final_CO_m36_status  final_CO_m36_sample_weight  \\\n",
       "ZEST_KEY                                                         \n",
       "76745_1_087_02            unseason                           1   \n",
       "76872_1_087_02            unseason                           1   \n",
       "77113_1_087_02            unseason                           1   \n",
       "77208_1_087_02            unseason                           1   \n",
       "77208_2_087_02            unseason                           1   \n",
       "77229_1_087_02            unseason                           1   \n",
       "77229_2_087_02            unseason                           1   \n",
       "77243_2_087_02            unseason                           1   \n",
       "77243_1_087_02            unseason                           1   \n",
       "77320_1_087_02            unseason                           1   \n",
       "77390_1_087_02            unseason                           1   \n",
       "77411_1_087_02            unseason                           1   \n",
       "77421_2_087_02            unseason                           1   \n",
       "77421_1_087_02            unseason                           1   \n",
       "\n",
       "               final_DateCO_m36 proxy_MSO_target_CO  proxy_CO  \\\n",
       "ZEST_KEY                                                        \n",
       "76745_1_087_02              NaT                 NaN       0.0   \n",
       "76872_1_087_02              NaT                 NaN       0.0   \n",
       "77113_1_087_02              NaT                 NaN       0.0   \n",
       "77208_1_087_02              NaT                 NaN       0.0   \n",
       "77208_2_087_02              NaT                 NaN       0.0   \n",
       "77229_1_087_02              NaT                 NaN       0.0   \n",
       "77229_2_087_02              NaT                 NaN       0.0   \n",
       "77243_2_087_02              NaT                 NaN       0.0   \n",
       "77243_1_087_02              NaT                 NaN       0.0   \n",
       "77320_1_087_02              NaT                 NaN       0.0   \n",
       "77390_1_087_02              NaT                 NaN       0.0   \n",
       "77411_1_087_02              NaT                 NaN       0.0   \n",
       "77421_2_087_02              NaT                 NaN       0.0   \n",
       "77421_1_087_02              NaT                 NaN       0.0   \n",
       "\n",
       "               final_MSO_target_CO  final_CO  \n",
       "ZEST_KEY                                      \n",
       "76745_1_087_02                 NaN       0.0  \n",
       "76872_1_087_02                 NaN       0.0  \n",
       "77113_1_087_02                 NaN       0.0  \n",
       "77208_1_087_02                 NaN       0.0  \n",
       "77208_2_087_02                 NaN       0.0  \n",
       "77229_1_087_02                 NaN       0.0  \n",
       "77229_2_087_02                 NaN       0.0  \n",
       "77243_2_087_02                 NaN       0.0  \n",
       "77243_1_087_02                 NaN       0.0  \n",
       "77320_1_087_02                 NaN       0.0  \n",
       "77390_1_087_02                 NaN       0.0  \n",
       "77411_1_087_02                 NaN       0.0  \n",
       "77421_2_087_02                 NaN       0.0  \n",
       "77421_1_087_02                 NaN       0.0  \n",
       "\n",
       "[14 rows x 188 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_df[target_df['final_DQ30_m3_status']=='unseason']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb302938-0c03-4202-8ad4-5a9d704971ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newest_model_engine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
