{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Notebook Explanation\n",
    "\n",
    "Evaluate score calibration for the final model.\n",
    "\n",
    "**Why do we need score calibration?**\n",
    "\n",
    "Our clients are used to evaluating credit risk based on the traditional FICO range of 300 to 850, with 300 representing the highest risk, 850 the lowest. The outputs of a traditional xgboost model can thus be quite confusing. This work by Moritz Becker serves to make our model outputs more digestible to clients and more comparable to the benchmark scores they typically use, such as FICO and Vantage scores. \n",
    "\n",
    "Additionally, a lot of the evaluations in this notebook can be thought of as a second evaluation of your model, not just of the calibration itself. If you have two models that score similarly, this notebook could be used to help choose between them.\n",
    "\n",
    "**How does it work?**\n",
    "\n",
    "This will be a bare-bones summary of how score calibration works. For more information, please visit https://zestfinance.atlassian.net/wiki/spaces/DS/pages/1623818294/Zest+Score+Calibration+and+implementation\n",
    "\n",
    "Score calibration uses two levels of mapping:\n",
    "\n",
    "1. Map score to risk: This step is specific to each client. Calculate XGBoost scores using the train scores for both national and client data. The scores are divided into 15 equally sized buckets for projects with >= 900k samples, using 1 fewer bucket for each 60k samples below that (with 10 buckets for cases with < 600k samples), and we calculate the mean target rate for each bucket (which we assume equals the mean risk). We then linearly interpolate between these mean target rate points. \n",
    "\n",
    "2. Map risk to calibrated score: This step uses one mapping for all clients and models at Zest. This step uses a fixed mapping, which maps risk scores to a score range from 300-850, where 720 represents risk odds of 40:1, and the odds double every 40 points.\n",
    "\n",
    "The score calibration artifacts for each model were automatically created when you built the model through model-engine. This notebook extracts those artifacts and evaluates the quality of that calibration. The following tests could point to an issue with either the calibration, or the model itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jag/.conda/envs/py310/lib/python3.10/site-packages/zaml/common/utils/io.py:17: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json \n",
    "import os\n",
    "import plotly.express as px\n",
    "\n",
    "from zaml.common.utils import load_state\n",
    "from zestio import load_data\n",
    "from model_engine.validators.zest_score_validation import MappingValidation, ScoreValidation, AUC_analysis\n",
    "from model_engine.assets.utils import load_asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define client and project name\n",
    "\n",
    "Use project_info.json to load our client and project name. Make sure to update the project_info.json file to match your client and project! \n",
    "\n",
    "Also specify the model_id for which you are performing score calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "californiacu equifax ['autoloan'] 1\n"
     ]
    }
   ],
   "source": [
    "with open('../model_iteration/autoloan/project_info.json', \"r\") as f:\n",
    "    project_info = json.load(f)\n",
    "    \n",
    "client = project_info['client_name']\n",
    "bureau = project_info['bureau_production']\n",
    "product_version = project_info['product_version']\n",
    "model_iteration_products = project_info['model_iteration_products']\n",
    "\n",
    "print(client, bureau, model_iteration_products, product_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to ProjectZ (V2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick reminder (collapsible):\n",
    "- Project Z hierarchy: client > dataset > model_iteration. Higher object need to be created/connected before lower object\n",
    "- `z_info` serves as a central access point to create, connect to, and manipulate ProjectZ objects\n",
    "- By this stage the client and dataset should be created by the data requesting team\n",
    "\n",
    "\n",
    "For more information about ProjectZ V2 object hierarchy and how to interact with `z_info` object, see this [user_guide](https://github.com/Katlean/projectz/blob/c61d8f312f0e687041b0271b7b7c4e3092ee3fd4/user_guide_v2.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:projectz.logger:Connected client: californiacu\n",
      "INFO:projectz.logger:Connected client: californiacu\n",
      "INFO:projectz.logger:Connected to client: californiacu\n"
     ]
    }
   ],
   "source": [
    "from projectz import ZInfo\n",
    "z_info = ZInfo() # Initialize z_info\n",
    "try:\n",
    "    z_info.connect_client(name = client) # connect to client level\n",
    "except ValueError as e:\n",
    "    print(f\"Client doesn't exist. Please double check the input or rerun notebook 0 to create new client\\nError message: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to dataset\n",
    "\n",
    "In the usual workflow, the arguments to connect to the dataset should be saved to `dataset_info.json` in notebook 0. \n",
    "Always confirm the dataset details printed in the cells below before connecting.\n",
    "If the configuration is outdated or incorrect, rerun Notebook 0 to create a new `dataset_info.json`.\n",
    "\n",
    "For more explanation for each argument, see the same section in `0_target_and_date_selection.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded Dataset Configuration:\n",
      "- model_type: ['underwriting']\n",
      "- Product: ['autoloan', 'personalloan', 'creditcard', 'homeequity']\n",
      "- Data Source: ['equifax']\n",
      "- Version: 1\n",
      "- Bureau for Modeling: equifax\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"../model_iteration/autoloan/dataset_info.json\", \"r\") as f:\n",
    "    dataset_info = json.load(f)\n",
    "\n",
    "dataset_model_type = dataset_info[\"dataset_model_type\"]\n",
    "dataset_data_source = dataset_info[\"dataset_data_source\"]\n",
    "dataset_product = dataset_info[\"dataset_product\"]\n",
    "dataset_version = dataset_info[\"dataset_version\"]\n",
    "bureau_modeling = dataset_info[\"bureau_modeling\"]\n",
    "\n",
    "# Sanity check\n",
    "print(f\"\"\"\\nLoaded Dataset Configuration:\n",
    "- model_type: {dataset_model_type}\n",
    "- Product: {dataset_product}\n",
    "- Data Source: {dataset_data_source}\n",
    "- Version: {dataset_version}\n",
    "- Bureau for Modeling: {bureau_modeling}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:projectz.logger:Connected dataset: autoloanCreditcardHomeequityPersonalloanv1\n",
      "INFO:projectz.logger:Connected dataset: autoloanCreditcardHomeequityPersonalloanv1\n",
      "INFO:projectz.logger:Connected to dataset: autoloanCreditcardHomeequityPersonalloanv1\n"
     ]
    }
   ],
   "source": [
    "# Connect to the dataset\n",
    "try:\n",
    "    z_info.connect_dataset(\n",
    "        model_type=dataset_model_type,\n",
    "        product=dataset_product,\n",
    "        version=dataset_version\n",
    "    )\n",
    "except ValueError:\n",
    "    print(\"Dataset not found with the given configuration. Please double check the settings or manually fix in the cell above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to model iteration\n",
    "\n",
    "By this stage you should be sure about the model iteration name. See the section in `1_run_model1.ipynb` for detailed explanation on model iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ATTENTION: set model iteration name\n",
    "model_iteration_name: str = 'model1_AppData_LTVAutoloan' # The model for which you are running score calibration\n",
    "assert model_iteration_name is not None, \"You have to set model iteration name\"\n",
    "\n",
    "# Uncomment this part if you want to search for the model iteration with matching product + version\n",
    "# matching_model_iterations = z_info.context.web_handler.get(f\"clients/{z_info.client.id}/datasets/{z_info.dataset.id}/model-iterations\")\n",
    "# for i, model_iter in enumerate(matching_model_iterations):\n",
    "#     print(f\"Available Model No. {i}: {json.dumps(model_iter, indent=4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See notebook `1_run_model.ipynb` for detailed explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PLEASE CHECK Model Iteration configuration:\n",
      "- Model Type: ['underwriting']\n",
      "- Data Source: ['equifax']\n",
      "- Product: ['autoloan']\n",
      "- Name: model1_AppData_LTVAutoloan\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ATTENTION: review and inspect model_iteration args; modify if necessary\n",
    "model_iteration_model_type: List[str] = dataset_model_type # usually same as dataset\n",
    "model_iteration_data_source: List[str] = dataset_data_source # usually same as dataset\n",
    "model_iteration_products: List[str] = model_iteration_products # Defined before. Usually a subset of dataset\n",
    "print(f\"\"\"\n",
    "PLEASE CHECK Model Iteration configuration:\n",
    "- Model Type: {model_iteration_model_type}\n",
    "- Data Source: {model_iteration_data_source}\n",
    "- Product: {model_iteration_products}\n",
    "- Name: {model_iteration_name}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:projectz.logger:Connected model_iteration: autoloan/model1_AppData_LTVAutoloan\n",
      "INFO:projectz.logger:Connected model_iteration: autoloan/model1_AppData_LTVAutoloan\n",
      "INFO:projectz.logger:Connected to model_iteration: autoloan/model1_AppData_LTVAutoloan\n"
     ]
    }
   ],
   "source": [
    "# if model iteration exist, connect\n",
    "# NOTE: you shoudn't need to create a new model iteration here. Please go back to notebook 1 if you want to run a new model\n",
    "try:\n",
    "    z_info.connect_model_iteration(\n",
    "        model_type=model_iteration_model_type,\n",
    "        product=model_iteration_products,\n",
    "        name=model_iteration_name\n",
    "    )\n",
    "except ValueError:\n",
    "    print(\"Dataset not found with the given configuration. Please check the settings above and rerun this cell if they were incorrect.\")\n",
    "    print(\"If this configuration is new, refer to projectz v2 user guide to create and connect new model iteration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get model strategy\n",
    "\n",
    "Specify if the model is a single or ensemble model, to help the validation objects know how to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_type:ensemble\n"
     ]
    }
   ],
   "source": [
    "modeling_model_artifacts_dir = z_info.model_iteration.paths['modeling_model_artifacts_dir']\n",
    "try:\n",
    "    with open(os.path.join(modeling_model_artifacts_dir, \"model_strategy.json\"), \"r\") as f:\n",
    "        model_strategy = json.load(f)\n",
    "        model_type = model_strategy['model_type']\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print('Using manual model_type input. Make sure you adjust this if required.')\n",
    "    # If there is no model_strategy.json, set it manually \n",
    "    model_type = \"single\"  # For manual input, options are 'single' or 'ensemble'\n",
    "    \n",
    "print(f'model_type:{model_type}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load score calibration artifacts**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running the mapping validation with benchmarking=True, the current validator will not accept any \"non-standard\" project names (i.e., \"autoloanv2\", etc.) by default. You can use a non-standard name by passing the standard project into the MappingValidation object, then passing the path to your actual project in the fit method.\n",
    "\n",
    "The following code uses the fact that typical \"non-standard\" names are simply a standard name plus a suffix. It will look for the presence of a \"standard\" project name within your current name, and if it finds one, will use that as the MappingValidation project. If your project name does not contain a \"standard\" project name, you will need to manually input it into the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using \"autoloan\" for mapping reference\n"
     ]
    }
   ],
   "source": [
    "# map validation\n",
    "map_projects = ['autoloan', 'personalloan', 'creditcard', 'allproducts']\n",
    "\n",
    "if len(model_iteration_products) == 1 and model_iteration_products[0] in map_projects: # standard case: single product + product found map_projects\n",
    "    print(f'Using \"{model_iteration_products[0]}\" for mapping reference')\n",
    "    product_calibration = model_iteration_products[0]\n",
    "else:\n",
    "    # either multiple products or product not found in map_projects: use allproducts\n",
    "    print(f\"Using allproducts for mapping reference of: {model_iteration_products}\")\n",
    "    product_calibration = \"allproducts\" # legacy name\n",
    "# TODO: add z_info back?\n",
    "mapval=MappingValidation(data_path=modeling_model_artifacts_dir,product=product_calibration,bureau=bureau,model_type=model_type)\n",
    "mapval.fit(benchmarking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score validation\n",
    "scoreval=ScoreValidation(data_path=modeling_model_artifacts_dir)\n",
    "scoreval.fit(benchmarking=False)\n",
    "\n",
    "# score comparison with benchmark\n",
    "scoreval_comp=ScoreValidation(data_path=modeling_model_artifacts_dir)\n",
    "scoreval_comp.fit(benchmarking=True)\n",
    "\n",
    "# auc validation\n",
    "auc_val=AUC_analysis(data_path=modeling_model_artifacts_dir)\n",
    "auc_val.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(model_iteration_products) > 1: # multiproduct\n",
    "    scoreval_by_prod={}\n",
    "    auc_val_by_prod={}\n",
    "    version_suffix = f\"v{product_version}\" if product_version > 1 else \"\"\n",
    "    for single_product in model_iteration_products:\n",
    "        file = os.path.join(\n",
    "            '/d/shared/silver_projects/project_power/shared_data/processed/client_data/', # client DB path\n",
    "            f\"{client}_{single_product}{version_suffix}\",\n",
    "            \"target.parquet\"\n",
    "        )\n",
    "        client_data = pd.read_parquet(file)\n",
    "\n",
    "        keys=client_data.index\n",
    "        scoreval_by_prod[single_product]=ScoreValidation(data_path=modeling_model_artifacts_dir)\n",
    "        scoreval_by_prod[single_product].fit(benchmarking=True, key_selection=keys)\n",
    "\n",
    "        auc_val_by_prod[single_product]=AUC_analysis(data_path=modeling_model_artifacts_dir)\n",
    "        auc_val_by_prod[single_product].fit(key_selection=keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at source [code](https://github.com/Katlean/model-engine/blob/06c863fd3dfd76f992584a28da0ae3ad01122876/model_engine/validators/zest_score_validation.py#L53C13-L75C1) here for how we actualy create the df for the compariison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "if benchmarking==True:\n",
    "\n",
    "    mapping_objs = load_asset('power/post_sale_path.json')['score_calibration_mapping_objects']\n",
    "\n",
    "    self.calib_risk_type_bureau_project = pd.read_pickle(os.path.join(mapping_objs, 'calib_risk_type_bureau_project.pkl'))\n",
    "    \n",
    "    self.e2e_mapping_type_bureau_project = pd.read_pickle(os.path.join(mapping_objs, 'e2e_mapping_type_bureau_project.pkl'))\n",
    "\n",
    "    comp_dic={\n",
    "    'experian':'exp', \n",
    "    'transunion':'TU', \n",
    "    'equifax':'efx'\n",
    "    }\n",
    "\n",
    "    bench_mapping=self.e2e_mapping_type_bureau_project[comp_dic[self.bureau]][self.product][self.model_type]\n",
    "    bench_mapping=bench_mapping.rename(columns={'score':'src', 'risk':'dstEstimatedRisk', 'calibrated':'dstRecalibratedScore'})\n",
    "    bench_mapping['Project']='Average across clients'\n",
    "\n",
    "    dfx=pd.concat([self.e2e_mapping,bench_mapping])\n",
    "    self.mapping_comparison_display=px.line(dfx,x='src',y='dstEstimatedRisk',color='Project')\n",
    "\n",
    "    psi_score=PSI()\n",
    "    psi_score.fit(self.e2e_mapping[['src']])\n",
    "    self.mapping_comparison_score=psi_score.transform(bench_mapping[['src']])[0]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_dic={\n",
    "'experian':'exp', \n",
    "'transunion':'TU', \n",
    "'equifax':'efx'\n",
    "}\n",
    "bench_mapping= mapval.e2e_mapping_type_bureau_project[comp_dic[mapval.bureau]][mapval.product][mapval.model_type]\n",
    "bench_mapping=bench_mapping.rename(columns={'score':'src', 'risk':'dstEstimatedRisk', 'calibrated':'dstRecalibratedScore'})\n",
    "bench_mapping['Project']='Average across clients'\n",
    "dfx=pd.concat([mapval.e2e_mapping,bench_mapping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('efx', 'autoloan', 'ensemble')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_dic[mapval.bureau], mapval.product, mapval.model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['efx', 'exp', 'TU'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapval.e2e_mapping_type_bureau_project.keys()\n",
    "## has a dictionary of mapping for each product buraue and model type combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_mapping = mapval.e2e_mapping_type_bureau_project['efx']['autoloan']['ensemble']\n",
    "bench_mapping=bench_mapping.rename(columns={'score':'src', 'risk':'dstEstimatedRisk', 'calibrated':'dstRecalibratedScore'})\n",
    "bench_mapping['Project']='Average across clients'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['src', 'dstEstimatedRisk', 'dstRecalibratedScore', 'Project'], dtype='object'),\n",
       " Index(['src', 'dstRecalibratedScore', 'dstEstimatedRisk', 'Project', 'diff'], dtype='object'))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bench_mapping.columns, mapval.e2e_mapping.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zaml.analyze.data_analysis.distribution_drift import PSI\n",
    "psi_score=PSI()\n",
    "psi_score.fit(mapval.e2e_mapping[['src']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mapp\u001b[0m/                          performance.parquet\n",
      "artifact_manifest.json        pipeline_equifax.obj\n",
      "asset.json                    pipeline_experian.obj\n",
      "calibration_object.obj        pipeline.obj\n",
      "client_predictor_config.json  pipeline_transunion.obj\n",
      "data_description.json         power_model_config.json\n",
      "feature_definition.parquet    run_time.json\n",
      "feature_importance.parquet    score_recalibration_mapping.json\n",
      "\u001b[01;34mfe_data\u001b[0m/                      splitter.obj\n",
      "inject_model_config.json      \u001b[01;34msubmodel_feature_importance\u001b[0m/\n",
      "input_configuration.json      submodel_performance.parquet\n",
      "keep_features.json            \u001b[01;34msubmodel_scores\u001b[0m/\n",
      "key_factors_mapping.json      \u001b[01;34mtarget\u001b[0m/\n",
      "model.obj                     test_data_summary.json\n",
      "model_strategy.json           unfold_data_description.json\n",
      "\u001b[01;34moverall_scores\u001b[0m/               value_based_key_factor_mapping.json\n",
      "\u001b[01;34moverall_zest_scores\u001b[0m/          versions.json\n"
     ]
    }
   ],
   "source": [
    "ls /d/shared/silver_projects_v2/californiacu/autoloanCreditcardHomeequityPersonalloanv1/modeling/model_artifacts/autoloan/model1_AppData_LTVAutoloan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/home/jag/.conda/envs/py310/lib/python3.10/site-packages/_distutils_hack/__init__.py:30: UserWarning:\n",
      "\n",
      "Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "base_path = '/d/shared/silver_projects_v2/californiacu/autoloanCreditcardHomeequityPersonalloanv1/modeling/model_artifacts/autoloan/model1_AppData_LTVAutoloan/'\n",
    "name = 'calibration_object.obj'\n",
    "\n",
    "def load_pickle(base_path, name):\n",
    "    path = f'{base_path}{name}'\n",
    "    with open(path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "        object = pickle.load(f)\n",
    "    return object\n",
    "calibration_object = load_pickle(base_path= base_path, name = name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<model_engine.model_builder.artifacts.zest_score.ZestScoreCalibration at 0x7f5a781d4520>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calibration_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['n_tiers', 'dst_risk_score_mapping', 'epsilon', 'src_min', 'src_max', 'dst_min', 'dst_max', 'dst_placeholders', 'src_placeholders', 'range_error', '_dst_to_intermediate_interf', '_intermediate_to_src_interf', '_src_to_intermediate_interf', '_intermediate_to_dst_interf', 'src_risk_score_mapping', '_src_quantile_mapping'])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calibration_object.__dict__.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calibration object has mapping here which for bucekts of 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.184825</td>\n",
       "      <td>0.000732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.187202</td>\n",
       "      <td>0.001965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.190311</td>\n",
       "      <td>0.003239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.194713</td>\n",
       "      <td>0.005828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.200892</td>\n",
       "      <td>0.009053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.209297</td>\n",
       "      <td>0.013471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.220272</td>\n",
       "      <td>0.021603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.234151</td>\n",
       "      <td>0.030412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.251303</td>\n",
       "      <td>0.046404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.272442</td>\n",
       "      <td>0.061813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.298949</td>\n",
       "      <td>0.088011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.334920</td>\n",
       "      <td>0.127041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.389272</td>\n",
       "      <td>0.196745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.494069</td>\n",
       "      <td>0.372328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       score      risk\n",
       "0   0.000000  0.000000\n",
       "1   0.184825  0.000732\n",
       "2   0.187202  0.001965\n",
       "3   0.190311  0.003239\n",
       "4   0.194713  0.005828\n",
       "5   0.200892  0.009053\n",
       "6   0.209297  0.013471\n",
       "7   0.220272  0.021603\n",
       "8   0.234151  0.030412\n",
       "9   0.251303  0.046404\n",
       "10  0.272442  0.061813\n",
       "11  0.298949  0.088011\n",
       "12  0.334920  0.127041\n",
       "13  0.389272  0.196745\n",
       "14  0.494069  0.372328\n",
       "15  1.000000  1.000000"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calibration_object._src_quantile_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key: n_tiers\n",
      "15\n",
      "\n",
      "\n",
      "key: dst_risk_score_mapping\n",
      "     score      risk\n",
      "550    300  1.000000\n",
      "549    301  0.649195\n",
      "548    302  0.648305\n",
      "547    303  0.647415\n",
      "546    304  0.646526\n",
      "..     ...       ...\n",
      "4      846  0.000104\n",
      "3      847  0.000069\n",
      "2      848  0.000040\n",
      "1      849  0.000016\n",
      "0      850  0.000000\n",
      "\n",
      "[551 rows x 2 columns]\n",
      "\n",
      "\n",
      "key: epsilon\n",
      "0.0001\n",
      "\n",
      "\n",
      "key: src_min\n",
      "0\n",
      "\n",
      "\n",
      "key: src_max\n",
      "1\n",
      "\n",
      "\n",
      "key: dst_min\n",
      "300\n",
      "\n",
      "\n",
      "key: dst_max\n",
      "850\n",
      "\n",
      "\n",
      "key: dst_placeholders\n",
      "[]\n",
      "\n",
      "\n",
      "key: src_placeholders\n",
      "[]\n",
      "\n",
      "\n",
      "key: range_error\n",
      "True\n",
      "\n",
      "\n",
      "key: _dst_to_intermediate_interf\n",
      "<scipy.interpolate._interpolate.interp1d object at 0x7f5a6cc4dda0>\n",
      "\n",
      "\n",
      "key: _intermediate_to_src_interf\n",
      "<scipy.interpolate._interpolate.interp1d object at 0x7f5a6cc4e2a0>\n",
      "\n",
      "\n",
      "key: _src_to_intermediate_interf\n",
      "<scipy.interpolate._interpolate.interp1d object at 0x7f5a6cc4e610>\n",
      "\n",
      "\n",
      "key: _intermediate_to_dst_interf\n",
      "<scipy.interpolate._interpolate.interp1d object at 0x7f5a6cc4e9d0>\n",
      "\n",
      "\n",
      "key: src_risk_score_mapping\n",
      "None\n",
      "\n",
      "\n",
      "key: _src_quantile_mapping\n",
      "       score      risk\n",
      "0   0.000000  0.000000\n",
      "1   0.184825  0.000732\n",
      "2   0.187202  0.001965\n",
      "3   0.190311  0.003239\n",
      "4   0.194713  0.005828\n",
      "5   0.200892  0.009053\n",
      "6   0.209297  0.013471\n",
      "7   0.220272  0.021603\n",
      "8   0.234151  0.030412\n",
      "9   0.251303  0.046404\n",
      "10  0.272442  0.061813\n",
      "11  0.298949  0.088011\n",
      "12  0.334920  0.127041\n",
      "13  0.389272  0.196745\n",
      "14  0.494069  0.372328\n",
      "15  1.000000  1.000000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in calibration_object.__dict__.keys():\n",
    "    print(f'key: {key}')\n",
    "    output = calibration_object.__dict__[key]\n",
    "    print(output)\n",
    "    print(f'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basically, when we build our calibration object, the first thing we do is we split our scores in 14 buckets of equal size\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## This code is found [here](https://github.com/Katlean/zaml/blob/1270c79eedb3129a3e042577efe0e373a41f1d6e/zaml/model/modeling/calibration.py#L610C4-L634C24) in the _quantiles_risk_estimator function in the FixedRiskScoreRecalibration Class in zaml which ZestScoreCalibration inherits from\n",
    "\n",
    "\n",
    "```python\n",
    "def _quantiles_risk_estimator(self, score, target, sc_min, sc_max):\n",
    "        d_scores = pd.DataFrame({\"score\": score, \"target\": target})\n",
    "        d_scores[\"tier\"] = pd.qcut(d_scores[\"score\"], q=self.n_tiers - 1, precision=10)\n",
    "        tiers = d_scores.groupby([\"tier\"]).mean().reset_index()\n",
    "        tiers.sort_values(by=[\"tier\"]).reset_index(drop=True)\n",
    "        risk_est = tiers.drop(\"tier\", inplace=False, axis=1)\n",
    "        risk_est.rename(columns={\"target\": \"risk\"}, inplace=True)\n",
    "\n",
    "        risk_est.sort_values(by=\"score\", inplace=True)\n",
    "        decreasing = (\n",
    "            risk_est.risk.iloc[0] > risk_est.risk.iloc[-1]\n",
    "        )  # if decreasing function target(score)\n",
    "        # add minimum\n",
    "        sc_min = min(score) if sc_min is None else sc_min\n",
    "        sc_min_row = pd.DataFrame(\n",
    "            [[sc_min, int(decreasing)]], columns=[\"score\", \"risk\"]\n",
    "        )\n",
    "        risk_est = pd.concat([sc_min_row, risk_est], ignore_index=True)\n",
    "        sc_max = max(score) if sc_max is None else sc_max\n",
    "        sc_max_row = pd.DataFrame(\n",
    "            [[sc_max, int(not decreasing)]], columns=[\"score\", \"risk\"]\n",
    "        )\n",
    "        risk_est = pd.concat([risk_est, sc_max_row], ignore_index=True)\n",
    "        risk_est[\"risk\"] = self._ensure_monotonicity(risk_est[\"risk\"], decreasing)\n",
    "        return risk_est\n",
    "```\n",
    "\n",
    "When we call [fit](https://github.com/Katlean/model-engine/blob/06c863fd3dfd76f992584a28da0ae3ad01122876/model_engine/model_builder/artifacts/zest_score.py#L33C2-L46C81) for our zest score calibration object\n",
    "\n",
    "```python\n",
    "  def fit(self, train_scores, train_target):\n",
    "        # train target or scores might be a list of dataframe/series or a single dataframe/series; we need to vertically concat them if they are in a list\n",
    "        train_target_union = pd.concat(train_target, axis=0) if isinstance(train_target, list) else train_target\n",
    "        train_scores_union = pd.concat(train_scores, axis=0) if isinstance(train_scores, list) else train_scores\n",
    "\n",
    "        train_target_union = train_target_union[train_target_union.notna()]\n",
    "        \n",
    "        mask = train_scores_union.index.intersection(train_target_union.index)\n",
    "        train_scores_union = train_scores_union.filter(items=mask, axis=0)\n",
    "        train_target_union = train_target_union.filter(items=mask, axis=0)\n",
    "\n",
    "        train_scores_union = train_scores_union.loc[train_target_union.index]\n",
    "        \n",
    "        super().fit(score_src=train_scores_union, target_src=train_target_union)\n",
    "```\n",
    "\n",
    "We call fit inherited which is FixedRiskScoreRecalibration which does not have a fit but inherits from ScoreRecalibrationBase\n",
    "\n",
    "which calls the [_fit](https://github.com/Katlean/zaml/blob/1270c79eedb3129a3e042577efe0e373a41f1d6e/zaml/model/modeling/calibration.py#L354C7-L359C10) which is in FixedRiskScoreRecalibration [here](https://github.com/Katlean/zaml/blob/1270c79eedb3129a3e042577efe0e373a41f1d6e/zaml/model/modeling/calibration.py#L669C5-L725C20)\n",
    "\n",
    "```python\n",
    " self._dst_quantile_mapping = self._quantiles_risk_estimator(\n",
    "                score_dst, target_dst, self.dst_min, self.dst_max\n",
    "            )\n",
    "```\n",
    "\n",
    "We can see this creates the mapping such it has 0 1 at the bounds and in between it has all the 14 and for each of those it has the average score and the average target rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.184825</td>\n",
       "      <td>0.000732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.187202</td>\n",
       "      <td>0.001965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.190311</td>\n",
       "      <td>0.003239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.194713</td>\n",
       "      <td>0.005828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.200892</td>\n",
       "      <td>0.009053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.209297</td>\n",
       "      <td>0.013471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.220272</td>\n",
       "      <td>0.021603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.234151</td>\n",
       "      <td>0.030412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.251303</td>\n",
       "      <td>0.046404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.272442</td>\n",
       "      <td>0.061813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.298949</td>\n",
       "      <td>0.088011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.334920</td>\n",
       "      <td>0.127041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.389272</td>\n",
       "      <td>0.196745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.494069</td>\n",
       "      <td>0.372328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       score      risk\n",
       "0   0.000000  0.000000\n",
       "1   0.184825  0.000732\n",
       "2   0.187202  0.001965\n",
       "3   0.190311  0.003239\n",
       "4   0.194713  0.005828\n",
       "5   0.200892  0.009053\n",
       "6   0.209297  0.013471\n",
       "7   0.220272  0.021603\n",
       "8   0.234151  0.030412\n",
       "9   0.251303  0.046404\n",
       "10  0.272442  0.061813\n",
       "11  0.298949  0.088011\n",
       "12  0.334920  0.127041\n",
       "13  0.389272  0.196745\n",
       "14  0.494069  0.372328\n",
       "15  1.000000  1.000000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calibration_object._src_quantile_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We then linearly intrpoalte between this to get everyone's estimate risk "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO get the zest score we then take that risk and map it into general_mapping.json to get the final risk score\n",
    "\n",
    "## this is fixed!!!\n",
    "\n",
    "That maps odds of roughly 45:1 which under the odds-doubling convention (40:1 at 720, doubling every 40 points) lands around a Zest score of ~725\n",
    "\n",
    "we can see it loaded [here](https://github.com/Katlean/model-engine/blob/06c863fd3dfd76f992584a28da0ae3ad01122876/model_engine/model_builder/artifacts/zest_score.py#L136C12-L136C72)\n",
    "\n",
    "```python\n",
    " dst_min, dst_max = int(min(map.keys())), int(max(map.keys()))\n",
    "\n",
    "        calib_score_range = np.arange(dst_max, dst_min-1, -1)\n",
    "        e2e_mapping=calibration_object.inverse_transform(calib_score_range)\n",
    "        e2e_mapping=e2e_mapping.rename(columns={'score':'src','risk':'dstEstimatedRisk','zest_score':'dstRecalibratedScore'})\n",
    "        e2e_mapping.sort_values('src',inplace=True)\n",
    "\n",
    "        score_recalibration_mapping=e2e_mapping[['src','dstRecalibratedScore','dstEstimatedRisk']].to_dict(orient='list')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thus that final e2e_mapping object we use in the PSI is abscialyl taking for every zest score what would have been the original model prediction to have generated that.\n",
    "\n",
    "## so when we do the PSI we are basically comparing hwo different the mapping is from original score to final zest score between our model and an average of models of the same type (national ensamble buraeu product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/home/jag/.conda/envs/py310/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning:\n",
      "\n",
      "This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jag/client-project-californiacu/autoloanCreditcardHomeequityPersonalloanv1/playground\n"
     ]
    }
   ],
   "source": [
    "cd playground/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_target_and_date_selection.ipynb\n",
      "claude_testing.ipynb\n",
      "\u001b[0m\u001b[01;34mclient-project-bestegg\u001b[0m/\n",
      "\u001b[01;34mclient-project-californiacu\u001b[0m/\n",
      "\u001b[01;34mclient-project-common\u001b[0m/\n",
      "\u001b[01;34mclient-project-members1stfcu\u001b[0m/\n",
      "\u001b[01;34mclient-project-patelcocu\u001b[0m/\n",
      "\u001b[01;34mclient-project-penfed\u001b[0m/\n",
      "\u001b[01;34mclient-project-westmark\u001b[0m/\n",
      "\u001b[01;34mcode\u001b[0m/\n",
      "Create_Model_Iteration_Onboarding_Guide.pdf\n",
      "delete_model_iteration_guide.md\n",
      "\u001b[01;34mdemoanalysis\u001b[0m/\n",
      "\u001b[01;34mdemoanalysis_backup_20260122_1751\u001b[0m/\n",
      "\u001b[01;34mfeature-engine-parts\u001b[0m/\n",
      "mega_model_config_analysis.pdf\n",
      "\u001b[01;34mmodel-engine\u001b[0m/\n",
      "model_engine_version_compatibility.md\n",
      "model_iteration_management_examples.py\n",
      "national_model_trace_log.md\n",
      "newest_model_engine_env_backup.yml\n",
      "\u001b[01;34mnewest_model_engine_kernelspec_backup\u001b[0m/\n",
      "\u001b[01;34mOnboarding\u001b[0m/\n",
      "\u001b[01;34mpresentable-engine\u001b[0m/\n",
      "\u001b[01;34mprojects\u001b[0m/\n",
      "\u001b[01;34mprojectz\u001b[0m/\n",
      "QUICK_ANSWER_Delete_Model_Iteration.txt\n",
      "QUICK_ANSWER_me_version_compatibility.txt\n",
      "QUICK_ANSWER_skip_argument_validation.txt\n",
      "README.md\n",
      "\u001b[01;34mreport-engine\u001b[0m/\n",
      "skip_argument_validation_explained.md\n",
      "\u001b[01;34mtest\u001b[0m/\n",
      "Testing_inquiry_and_trade_dables.ipynb\n",
      "Untitled10.ipynb\n",
      "Untitled1.ipynb\n",
      "untitled1.txt\n",
      "Untitled2.ipynb\n",
      "Untitled3.ipynb\n",
      "Untitled4.ipynb\n",
      "Untitled5.ipynb\n",
      "Untitled6.ipynb\n",
      "Untitled7.ipynb\n",
      "Untitled8.ipynb\n",
      "Untitled9.ipynb\n",
      "Untitled.ipynb\n",
      "untitled.txt\n",
      "view_s3_files_guide.py\n",
      "Walk_Through_Client_Model_3a.ipynb\n",
      "\u001b[01;34mwork\u001b[0m/\n",
      "\u001b[01;34mzaml_test\u001b[0m/\n",
      "\u001b[01;34mzestio\u001b[0m/\n",
      "\u001b[01;34mzpower\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "california_cu_v1",
   "language": "python",
   "name": "california_cu_v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
